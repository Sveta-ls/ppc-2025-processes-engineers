## Параллельный метод построения выпуклой оболочки (проход Грэхема)

- Student: <Зязева Светлана Александровна>, group <3823Б1ПР2>
- Technology: <SEQ | MPI>
- Variant: <24>

---

## 1. Introduction

Построение выпуклой оболочки множества точек — одна из базовых задач вычислительной геометрии. Выпуклая оболочка представляет собой минимальный выпуклый многоугольник, содержащий все заданные точки. Задача широко применяется в компьютерной графике, анализе изображений, геоинформационных системах и моделировании.

Алгоритм Грэхема — алгоритм построения выпуклой оболочки в двумерном пространстве. 
В этом алгоритме задача о выпуклой оболочке решается с помощью стека, сформированного из точек-кандидатов. Все точки входного множества заносятся в стек, а потом точки, не являющиеся вершинами выпуклой оболочки, со временем удаляются из него.

---

## 2. Problem Statement

Требуется реализовать алгоритм построения выпуклой оболочки для множества точек на плоскости.
Входные данные — вектор точек P={(xi​,yi​)}, где каждая точка задаётся целочисленными координатами.

Алгоритм должен:

- Отсортировать точки по координате x (при равенстве — по y)

- Построить нижнюю и верхнюю цепи выпуклой оболочки.

- Исключить внутренние точки.

- Вернуть упорядоченный набор вершин выпуклой оболочки.

### Ограничения и требования:

- Если количество входных точек меньше 3, результатом является пустое множество.
- Алгоритм должен корректно обрабатывать коллинеарные точки.

---

## 3. Baseline Algorithm (Sequential)

Последовательная версия алгоритма основана на модификации алгоритма Грэхема. Основные этапы:

- Сортировка всех точек по координатам.
- Последовательное построение нижней оболочки с удалением точек, нарушающих выпуклость.
- Аналогичное построение верхней оболочки.
- Объединение результатов в итоговую выпуклую оболочку.

```cpp
std::vector<Point> BuildConvexHull(std::vector<Point> pts) {
  std::ranges::sort(pts.begin(), pts.end(),
                    [](const Point& a, const Point& b) { return a.x < b.x || (a.x == b.x && a.y < b.y); });

  std::vector<Point> hull;

  for (const auto& p : pts) {
    while (hull.size() >= 2 && Cross(hull[hull.size() - 2], hull.back(), p) <= 0) {
      hull.pop_back();
    }
    hull.push_back(p);
  }

  std::size_t lower_size = hull.size();
  for (int i = static_cast<int>(pts.size()) - 2; i >= 0; --i) {
    const auto& p = pts[i];
    while (hull.size() > lower_size && Cross(hull[hull.size() - 2], hull.back(), p) <= 0) {
      hull.pop_back();
    }
    hull.push_back(p);
  }

  hull.pop_back();
  return hull;
}

bool ZyazevaSGrahamSchemeSEQ::RunImpl() {
  const auto& points = GetInput();

  if (points.size() < 3) {
    GetOutput().clear();
    return true;
  }

  GetOutput() = BuildConvexHull(points);
  return true;
}
```
---

## 4. Parallelization Scheme

Параллельная версия алгоритма реализована с использованием MPI и основана на идее локального построения оболочек с последующим их объединением.

1. **Инициализация MPI:**
   - Каждому процессу присваивается уникальный ранг `rank` и выясняется общее количество процессов `size`.
2. **Разделение данных:**  
   - Входной массив точек делится между процессами таким образом, чтобы каждый процесс получил приблизительно одинаковое количество точек.
3. **Цикл по столбцам:**  
   - Каждый процесс независимо строит выпуклую оболочку для своего подмножества точек с использованием последовательного алгоритма. 
   - Локальные оболочки попарно передаются и объединяются:
    - принимающий процесс объединяет две оболочки
    - повторно строит выпуклую оболочку для объединённого множества точек
4. **Сбор результатов (MPI_Gatherv):**  
   - Процесс 0 сохраняет итоговую выпуклую оболочку как результат выполнения алгоритма.
5. **Итог: Вывод результата и завершение работы MPI.**

---

## 5. Experimental Setup

**Тестовое окружение**
- Процессор и операционная система: AMD Ryzen 7 8845HS, 4 ядра, 32 GB RAM, Windows 11 pro x64
- Инструменты:
    - Cmake 3.28.3
    - Компилятор: g++ (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0
    - Использовался Docker-контейнер.
- Данные: Для замера производительности использовались векторы точек размером 10 000 000, 25 000 000, 50 000 000.

---

## 6. Results and Discussion

### Размер вектора с точками \(10 000 000\)

| Версия выполнения | Время (секунд) | Количество процессов|
|:-----------------|-------------:|-----------------:|
| SEQ pipeline | 0.3891 | 1 |
| MPI pipeline | 0.0940 | 4 | 
| MPI pipeline | 0.0760 | 8 | 
| SEQ task_run | 0.3876 | 1 | 
| MPI task_run | 0.1061 | 4 | 
| MPI task_run | 0.0664 | 8 | 

MPI быстрее SEQ в 3.6-4.1 раза при запуске на 4 процессах, в 5.1-5.8 раз при запуске на 8.

### Размер вектора с точками \(25 000 000\)

| Версия выполнения | Время (секунд) | Количество процессов|
|:-----------------|-------------:|-----------------:|
| SEQ pipeline | 1.1059 | 1 |
| MPI pipeline | 0.2648 | 4 |
| MPI pipeline | 0.1762 | 8 |  
| SEQ task_run | 1.0567 | 1 | 
| MPI task_run | 0.2692 | 4 | 
| MPI task_run | 0.1719 | 8 | 

MPI быстрее SEQ в в 3.9-4.1 раз при запуске на 4 процессах, в 6.2-6.1 раза при запуске на 8.

### Размер вектора с точками \(50 000 000\)

| Версия выполнения | Время (секунд) | Количество процессов|
|:-----------------|-------------:|-----------------:|
| SEQ pipeline | 2.3231 | 1 |
| MPI pipeline | 0.5599 | 4 |
| MPI pipeline | 0.4463 | 8 |  
| SEQ task_run | 2.4389 | 1 | 
| MPI task_run | 0.5393 | 4 | 
| MPI task_run | 0.3955 | 8 | 

MPI быстрее SEQ в в 4.1-4.5 раз при запуске на 4 процессах, в 5.2- раза при запуске на 8.

**Вывод:** Параллельная реализация успешно решает поставленную задачу — обеспечивает ускорение более чем в 4 раза при обработке большого множества точек, что подтверждает целесообразность использования MPI построения выпуклой оболочки.

---

## 7. Conclusions
1. Реализованы последовательная и параллельная версии алгоритма построения выпуклой оболочки.
2. Параллельная реализация с использованием MPI демонстрирует заметное ускорение при больших объёмах входных данных.
3. Полученные результаты подтверждают эффективность распределённого подхода для задач вычислительной геометрии.

---

## 8. References

1. Курс лекций "Параллельное программирование для кластерных систем"
2. Функции MPI. - https://learn.microsoft.com/ru-ru/message-passing-interface/mpi-functions
3. Документация по курсу: https://learning-process.github.io/parallel_programming_course/ru