## Параллельное вычисление скалярного произведения векторов с использованием MPI

- Student: <Зязева Светлана Александровна>, group <3823Б1ПР2>
- Technology: <SEQ | MPI>
- Variant: <9>

---

## 1. Introduction

Скалярное произведение векторов — это важная операция в математике, которую часто используют в науке, компьютерах и графике. Когда нужно работать с длинными векторами, обычные способы вычислений становятся слишком медленными. Поэтому применяют параллельные вычисления, которые позволяют выполнять несколько задач одновременно, ускоряя процесс.

---

## 2. Problem Statement

Требуется вычислить скалярное произведение двух векторов **A** и **B** одинаковой размерности **N** по формуле:

$$
\mathbf{A} \cdot \mathbf{B} = \sum_{i=0}^{N-1} A[i] \times B[i]
$$

### Ограничения и требования:
- Векторы должны иметь одинаковую размерность
- Элементы векторов — целочисленные значения

---

## 3. Baseline Algorithm (Sequential)

Последовательный алгоритм вычисления скалярного произведения реализован следующим образом:

```cpp
bool ZyazevaSVecDotProductSEQ::RunImpl() {
  auto &input = GetInput();
  auto &vec1 = input[0];
  auto &vec2 = input[1];

  int dot_product = 0;

  for (size_t i = 0; i < vec1.size(); i++) {
    dot_product += vec1[i] * vec2[i];
  }

  GetOutput() = dot_product;
  return true;
}
```
---

## 4. Parallelization Scheme

Параллельный алгоритм вычисления скалярного произведения реализован следующим образом:

1. **Инициализация MPI** — Каждому процессу присваивается уникальный ранг `rank` и выясняется общее количество процессов `size`.
2. **Разделение данных:**  
   - Вектора равномерно распределяются между процессами.  
   - Если длина векторов не кратна количеству процессов, избыточные элементы распределяются между первыми процессами.  
3. **Вычисление частичных результатов:**  
   Каждый процесс выполняет операцию скалярного произведения над своей частью векторов.  
4. **Сбор результатов:**  
   Для вычисления суммы частичных произведений всех процессов с последующим распределением итогового значения на каждый узел применяется операция `MPI_Allreduce`.
5. **Итог: Вывод результата и завершение работы MPI.**

---

## 5. Experimental Setup

**Тестовое окружение**
- Процессор и операционная система: AMD Ryzen 7 8845HS, 4 ядра, 32 GB RAM, Windows 11 pro x64
- Инструменты:
    - Cmake 3.28.3
    - Компилятор: g++ (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0
    - Использовался Docker-контейнер.
- Данные: Для замера производительности использовались векторы размером 1000000

---

## 6. Results and Discussion

### Размер векторов \(25000000\)

| Версия выполнения | Время (секунд) | Количество процессов|
|:-----------------|-------------:|-----------------:|
| SEQ pipeline | 0.0339 | 1 |
| MPI pipeline | 0.1609 | 4 | 
| SEQ task_run | 0.0330 | 1 | 
| MPI task_run | 0.1567 | 4 | 

MPI медленнее SEQ и в task_run и в pipeline.
**Вывод:** Из-за накладных расходов на работу с процессами и рассылку данных MPI версия работает 4.5-5 раз медленнее, чем SEQ, поэтому менеее эффективна.

---

## 7. Conclusions
1. Созданы две реализации алгоритма: последовательная и с использованием MPI.  
2. При небольших объёмах данных затраты на использование MPI становятся выше, чем преимущества от распараллеливания.
3. Даже при увеличении объёмов данных MPI-версия не показывает ускорение.

---

## 8. References

1. Курс лекций "Параллельное программирование для кластерных систем"
2. Функции MPI. - https://learn.microsoft.com/ru-ru/message-passing-interface/mpi-functions